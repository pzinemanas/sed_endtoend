from keras.layers import Input, Conv2D, MaxPooling2D,Conv1D,MaxPooling1D,AveragePooling1D
from keras.layers import Dense, Flatten,Lambda,Activation,TimeDistributed,LocallyConnected1D,Concatenate,LocallyConnected2D
from keras.layers.core import Dropout
from keras.models import Model
from keras.layers.normalization import BatchNormalization
from keras import backend as K
from keras.regularizers import Regularizer

class L1L2(Regularizer):
    """Regularizer for L1 and L2 regularization.
    # Arguments
        l1: Float; L1 regularization factor.
        l2: Float; L2 regularization factor.
    """

    def __init__(self, l1=0., l2=0., filters=0.):
        self.l1 = K.cast_to_floatx(l1)
        self.l2 = K.cast_to_floatx(l2)
        self.filters = K.cast_to_floatx(filters)

    def __call__(self, x):
        regularization = 0.
        if self.l1:
            regularization += K.sum(self.l1 * K.abs((x-self.filters)/(self.filters+0.000001)))
        if self.l2:
            regularization += K.sum(self.l2 * K.square((x-self.filters)/(self.filters+0.000001)))
        return regularization

    def get_config(self):
        return {'l1': float(self.l1),'l2': float(self.l2)}


def MST(N_mels,sequence_samples,audio_win,audio_hop):#, filters):

    x = Input(shape=(sequence_samples,1), dtype='float32') #(1025,50)

    print('entrada',x.shape)
    y = Conv1D(512,audio_win, strides=audio_hop, padding='same')(x)
    y = BatchNormalization()(y)
    y = Activation('relu')(y)
    print('conv1',x.shape)
	 
    y = Conv1D(256,3, strides=1, padding='same')(y)
    y = BatchNormalization()(y)
    y = Activation('relu')(y)
    print('conv2',x.shape)
    
    y = Conv1D(N_mels,3, strides=1, padding='same')(y)
    y = BatchNormalization()(y)
    y = Activation('tanh')(y)
    print('output',x.shape)

   # y = Lambda(lambda x: K.permute_dimensions(x, (0,2,1)))(y)
   # y = Lambda(lambda x: K.expand_dims(x, axis=-1))(y)

    m = Model(inputs=x, outputs=y)

    return m


def MELextraction(N_mels,wins,audio_win,audio_hop,alpha=1,scaler=None,amin=1e-10,filters=None):
    x = Input(shape=(wins,audio_win,1), dtype='float32') #(1025,50)
    
    reg = L1L2(0.0,0.0001,filters)

    y = TimeDistributed(Conv1D(N_mels,1024, strides=16, padding='same',use_bias=True))(x)#strides 64 kernel_regularizer=reg use_bias=False

    y = Lambda(lambda x: x*x)(y)

    y = Lambda(lambda x: audio_win*K.mean(x,axis=2))(y)

    #y = Activation('relu')(y)

    y = Lambda(lambda x: 10*K.log(K.maximum(amin, x*alpha))/K.log(10.))(y)

    if scaler is not None:
        y = Lambda(lambda x: 2*((x-scaler[0])/(scaler[1]-scaler[0])-0.5))(y)

    m = Model(inputs=x, outputs=y)
    
    return m

def build_custom_cnn(n_freq_cnn=128, n_frames_cnn=50, n_filters_cnn=64,
                     filter_size_cnn=(5, 5), pool_size_cnn=(2,2),
                     n_classes=10, large_cnn=False, n_dense_cnn=64,gamma=1,beta=0):

    if large_cnn:
        n_filters_cnn = 128
        n_dense_cnn = 128

    # INPUT
    x = Input(shape=(n_frames_cnn,n_freq_cnn), dtype='float32')
    
    #y = Lambda(lambda x: K.permute_dimensions(x, (0,2,1,3)))(y)    
    
    #y = Lambda(lambda x: (x - beta)/gamma )(x)
    y = BatchNormalization()(x)
    
    y = Lambda(lambda x: K.expand_dims(x,-1))(y) 
    
#    y = Dropout(0.5)(x)
#    x_dummy = Input(shape=(n_frames_cnn,n_freq_cnn), dtype='float32')
    #y = BatchNormalization(axis=1)(x)
    # CONV 1
    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',
               activation='relu')(y)
    y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)
    y = BatchNormalization()(y)

    # CONV 2
    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',
               activation='relu')(y)
    y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)
    y = BatchNormalization()(y)

    # CONV 3
    y = Conv2D(n_filters_cnn, filter_size_cnn, padding='valid',
               activation='relu')(y)
    # y = MaxPooling2D(pool_size=pool_size_cnn, strides=None, padding='valid')(y)
    y = BatchNormalization()(y)

    # Flatten for dense layers
    y = Flatten()(y)
    y = Dropout(0.5)(y)
    y = Dense(n_dense_cnn, activation='relu')(y)
    if large_cnn:
        y = Dropout(0.5)(y)
        y = Dense(n_dense_cnn, activation='relu')(y)
    y = Dropout(0.5)(y)
    y = Dense(n_classes, activation='sigmoid')(y)

    m = Model(inputs=x, outputs=y)
#    print(m.layers[1].get_weights()[0].shape)
#    m = Model(inputs=x, outputs=y)
    return m
    
def concatenate(wins,audio_win,model_cnn,model_mel,sequence_samples=22050,frames=True):
    if frames:
        x = Input(shape=(wins,audio_win,1), dtype='float32')
    else:
        x = Input(shape=(sequence_samples,1), dtype='float32')        
    y_mel = model_mel(x)
    y = model_cnn(y_mel)
    m = Model(x,[y,y_mel])
    return m
    
    
from keras import regularizers,constraints
from keras.engine.topology import Layer

class MyLayer(Layer):

    def __init__(self, output_dim, filters, **kwargs):
        self.output_dim = output_dim
        self.reg = L1L2(0.0,10.,filters)
        super(MyLayer, self).__init__(**kwargs)


    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel', 
                                      shape=(input_shape[2], self.output_dim),
                                      initializer='uniform',
                                      #regularizer=regularizers.l2(0.0001),
                                      regularizer=self.reg,
                                      constraint = constraints.non_neg(),
                                      trainable=True)
        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, x):
        return K.dot(x, self.kernel)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[1], self.output_dim)

    def get_config(self):
        config = {'output_dim': self.output_dim}
        base_config = super(MyLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))    



def MELextraction_stft(N_mels,N_hops,N_freqs,scaler=None,amin=1e-10,alpha=1,filters=None):#, filters):

    x = Input(shape=(N_hops,N_freqs), dtype='float32') #(1025,50)
    x1 = Lambda(lambda x: K.expand_dims(x))(x)    
    reg = L1L2(0.0,0.00001,filters)    
    y = TimeDistributed(LocallyConnected1D(N_mels,N_freqs,strides=N_freqs,use_bias=False,kernel_constraint=constraints.non_neg()))(x1) #kernel_regularizer=reg,
    
#    
#    y = []
#    for j in range(N_mels):
#        y.append(TimeDistributed(Conv1D(1,N_freqs,padding="valid",strides=N_freqs,use_bias=False))(x1))
#        
#    y = Concatenate()(y)
    y = Lambda(lambda x: K.squeeze(x,2))(y)  
    print(y.shape)
    
    
    
#    layer = MyLayer(output_dim=N_mels,filters=filters)
#    y = layer(x)

    y = Lambda(lambda x: 10*K.log(K.maximum(amin, x*alpha))/K.log(10.))(y)

    if scaler is not None:
        y = Lambda(lambda x: 2*((x-scaler[0])/(scaler[1]-scaler[0])-0.5))(y)

    m = Model(inputs=x, outputs=y)
    
    return m
    
def concatenate_stft(N_hops,N_freqs,model_cnn,model_mel,sequence_samples=22050,frames=True):

    x = Input(shape=(N_hops,N_freqs), dtype='float32')    
    y_mel = model_mel(x)
    y = model_cnn(y_mel)
    m = Model(x,[y,y_mel])
    return m