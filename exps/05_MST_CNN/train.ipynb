{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MST+CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'../..')\n",
    "from sed_endtoend.cnn.model import build_custom_cnn\n",
    "from sed_endtoend.mst.model import MST\n",
    "from sed_endtoend.callbacks import MetricsCallback\n",
    "from sed_endtoend.concatenate_models import concatenate\n",
    "from sed_endtoend.data_generator import DataGenerator, Scaler\n",
    "#from sed_endtoend.gen_mel_filters import mel_filters\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# files parameters\n",
    "Nfiles = None\n",
    "resume = False\n",
    "load_subset = Nfiles\n",
    "\n",
    "# load parameters\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founding scaler\n",
      "Making training generator\n",
      "Making validation generator\n",
      "Getting data\n",
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n",
      "0.0 %\n",
      "10.0 %\n",
      "20.0 %\n",
      "30.0 %\n",
      "40.0 %\n",
      "50.0 %\n",
      "60.0 %\n",
      "70.0 %\n",
      "80.0 %\n",
      "90.0 %\n"
     ]
    }
   ],
   "source": [
    "params = {'sequence_time': sequence_time, 'sequence_hop_time':sequence_hop_time,\n",
    "          'label_list':label_list,'audio_hop':audio_hop, 'audio_win':audio_win,\n",
    "          'n_fft':n_fft,'sr':sr,'mel_bands':mel_bands,'normalize':normalize_data, \n",
    "          'frames':frames,'get_annotations':get_annotations, 'dataset': dataset}\n",
    "\n",
    "sequence_frames = int(np.ceil(sequence_time*sr/audio_hop))\n",
    "\n",
    "# Datasets\n",
    "labels = {}# Labels\n",
    "\n",
    "train_files = sorted(glob.glob(os.path.join(audio_folder,'train', '*.wav')))\n",
    "val_files = sorted(glob.glob(os.path.join(audio_folder,'validate', '*.wav')))\n",
    "\n",
    "if load_subset is not None:\n",
    "    train_files = train_files[:load_subset]\n",
    "    val_files = val_files[:load_subset]\n",
    "\n",
    "train_labels = {}\n",
    "train_mel = {}\n",
    "val_labels = {}\n",
    "val_mel = {}\n",
    "\n",
    "for n,id in enumerate(train_files):\n",
    "    labels[id] = os.path.join(label_folder, 'train',os.path.basename(id).replace('.wav','.txt'))\n",
    "for id in val_files:\n",
    "    labels[id] = os.path.join(label_folder, 'validate',os.path.basename(id).replace('.wav','.txt'))\n",
    "\n",
    "# Generators\n",
    "print('Making training generator')\n",
    "training_generator = DataGenerator(train_files, labels, **params)\n",
    "\n",
    "params['sequence_hop_time'] = sequence_time # To calculate F1_1s\n",
    "\n",
    "print('Making validation generator')\n",
    "validation_generator = DataGenerator(val_files, labels, **params)\n",
    "\n",
    "print('Getting data')\n",
    "x_val,_,mel_val,y_val = validation_generator.return_all()\n",
    "x_train,_,mel_train,y_train = training_generator.return_all()\n",
    "\n",
    "print('Founding scaler')\n",
    "scaler = Scaler(normalizer=normalize_data)\n",
    "scaler.fit(mel_train)\n",
    "mel_train = scaler.transform(mel_train)\n",
    "mel_val = scaler.transform(mel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1003 11:38:21.972408 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1003 11:38:21.987283 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1003 11:38:21.989398 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1003 11:38:22.040526 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 11:38:22.232811 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1003 11:38:22.242889 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1003 11:38:27.990638 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1003 11:38:28.255285 139944068986624 deprecation.py:506] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1003 11:38:29.073240 139944068986624 deprecation_wrapper.py:119] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1003 11:38:29.080075 139944068986624 deprecation.py:323] From /home/pzinemanas/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 22050, 1)          0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 44, 128)           1020288   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 10)                2481162   \n",
      "=================================================================\n",
      "Total params: 3,501,450\n",
      "Trainable params: 3,498,634\n",
      "Non-trainable params: 2,816\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting model...\n",
      "Epoch 1/101\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.2539 - model_2_loss: 0.2814 - model_1_loss: 0.0062\n",
      "F1 = 0.3790, ER = 0.7390 -  Best val F1s: 0.3790 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 2/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2305 - model_2_loss: 0.2554 - model_1_loss: 0.0069\n",
      "F1 = 0.4056, ER = 0.7147 -  Best val F1s: 0.4056 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 3/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2238 - model_2_loss: 0.2478 - model_1_loss: 0.0075\n",
      "F1 = 0.4105, ER = 0.7062 -  Best val F1s: 0.4105 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 4/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2205 - model_2_loss: 0.2441 - model_1_loss: 0.0080\n",
      "F1 = 0.4105, ER = 0.7103 - Best val F1s: 0.4105 (2)\n",
      "\n",
      "Epoch 5/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.2162 - model_2_loss: 0.2393 - model_1_loss: 0.0084\n",
      "F1 = 0.4134, ER = 0.7078 -  Best val F1s: 0.4134 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 6/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.2129 - model_2_loss: 0.2356 - model_1_loss: 0.0084\n",
      "F1 = 0.4244, ER = 0.6980 -  Best val F1s: 0.4244 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 7/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.2113 - model_2_loss: 0.2338 - model_1_loss: 0.0087\n",
      "F1 = 0.4280, ER = 0.6910 -  Best val F1s: 0.4280 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 8/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2095 - model_2_loss: 0.2318 - model_1_loss: 0.0087\n",
      "F1 = 0.4216, ER = 0.6946 - Best val F1s: 0.4280 (6)\n",
      "\n",
      "Epoch 9/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2060 - model_2_loss: 0.2279 - model_1_loss: 0.0089\n",
      "F1 = 0.4514, ER = 0.6629 -  Best val F1s: 0.4514 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 10/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2051 - model_2_loss: 0.2269 - model_1_loss: 0.0090\n",
      "F1 = 0.4352, ER = 0.6831 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 11/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.2023 - model_2_loss: 0.2237 - model_1_loss: 0.0093\n",
      "F1 = 0.4314, ER = 0.6860 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 12/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.2004 - model_2_loss: 0.2216 - model_1_loss: 0.0092\n",
      "F1 = 0.4424, ER = 0.6718 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 13/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1986 - model_2_loss: 0.2196 - model_1_loss: 0.0093\n",
      "F1 = 0.4316, ER = 0.6801 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 14/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1965 - model_2_loss: 0.2173 - model_1_loss: 0.0092\n",
      "F1 = 0.4391, ER = 0.6709 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 15/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1949 - model_2_loss: 0.2155 - model_1_loss: 0.0094\n",
      "F1 = 0.4457, ER = 0.6645 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 16/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1927 - model_2_loss: 0.2131 - model_1_loss: 0.0094\n",
      "F1 = 0.4491, ER = 0.6606 - Best val F1s: 0.4514 (8)\n",
      "\n",
      "Epoch 17/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1902 - model_2_loss: 0.2102 - model_1_loss: 0.0096\n",
      "F1 = 0.4594, ER = 0.6480 -  Best val F1s: 0.4594 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 18/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1889 - model_2_loss: 0.2088 - model_1_loss: 0.0094\n",
      "F1 = 0.4587, ER = 0.6474 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 19/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1869 - model_2_loss: 0.2066 - model_1_loss: 0.0098\n",
      "F1 = 0.4439, ER = 0.6665 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 20/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1858 - model_2_loss: 0.2053 - model_1_loss: 0.0098\n",
      "F1 = 0.4507, ER = 0.6582 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 21/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1841 - model_2_loss: 0.2034 - model_1_loss: 0.0098\n",
      "F1 = 0.4462, ER = 0.6622 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 22/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1827 - model_2_loss: 0.2019 - model_1_loss: 0.0099\n",
      "F1 = 0.4563, ER = 0.6476 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 23/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1805 - model_2_loss: 0.1995 - model_1_loss: 0.0100\n",
      "F1 = 0.4378, ER = 0.6599 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 24/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1797 - model_2_loss: 0.1986 - model_1_loss: 0.0100\n",
      "F1 = 0.4547, ER = 0.6477 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 25/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1771 - model_2_loss: 0.1957 - model_1_loss: 0.0100\n",
      "F1 = 0.4550, ER = 0.6446 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 26/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1762 - model_2_loss: 0.1947 - model_1_loss: 0.0101\n",
      "F1 = 0.4473, ER = 0.6590 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 27/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1741 - model_2_loss: 0.1923 - model_1_loss: 0.0103\n",
      "F1 = 0.4362, ER = 0.6740 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 28/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1726 - model_2_loss: 0.1907 - model_1_loss: 0.0103\n",
      "F1 = 0.4526, ER = 0.6552 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 29/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1725 - model_2_loss: 0.1905 - model_1_loss: 0.0103\n",
      "F1 = 0.4539, ER = 0.6494 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 30/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1704 - model_2_loss: 0.1882 - model_1_loss: 0.0101\n",
      "F1 = 0.4578, ER = 0.6383 - Best val F1s: 0.4594 (16)\n",
      "\n",
      "Epoch 31/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1683 - model_2_loss: 0.1858 - model_1_loss: 0.0101\n",
      "F1 = 0.4663, ER = 0.6255 -  Best val F1s: 0.4663 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 32/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1675 - model_2_loss: 0.1850 - model_1_loss: 0.0104\n",
      "F1 = 0.4528, ER = 0.6473 - Best val F1s: 0.4663 (30)\n",
      "\n",
      "Epoch 33/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1647 - model_2_loss: 0.1818 - model_1_loss: 0.0104\n",
      "F1 = 0.4462, ER = 0.6498 - Best val F1s: 0.4663 (30)\n",
      "\n",
      "Epoch 34/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1641 - model_2_loss: 0.1812 - model_1_loss: 0.0105\n",
      "F1 = 0.4558, ER = 0.6465 - Best val F1s: 0.4663 (30)\n",
      "\n",
      "Epoch 35/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1630 - model_2_loss: 0.1799 - model_1_loss: 0.0106\n",
      "F1 = 0.4573, ER = 0.6386 - Best val F1s: 0.4663 (30)\n",
      "\n",
      "Epoch 36/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1610 - model_2_loss: 0.1777 - model_1_loss: 0.0105\n",
      "F1 = 0.4648, ER = 0.6311 - Best val F1s: 0.4663 (30)\n",
      "\n",
      "Epoch 37/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1599 - model_2_loss: 0.1765 - model_1_loss: 0.0104\n",
      "F1 = 0.4743, ER = 0.6120 -  Best val F1s: 0.4743 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 38/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1582 - model_2_loss: 0.1746 - model_1_loss: 0.0106\n",
      "F1 = 0.4641, ER = 0.6253 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 39/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1584 - model_2_loss: 0.1748 - model_1_loss: 0.0107\n",
      "F1 = 0.4539, ER = 0.6455 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 40/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1565 - model_2_loss: 0.1727 - model_1_loss: 0.0106\n",
      "F1 = 0.4571, ER = 0.6377 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 41/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1545 - model_2_loss: 0.1705 - model_1_loss: 0.0106\n",
      "F1 = 0.4518, ER = 0.6258 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 42/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1548 - model_2_loss: 0.1708 - model_1_loss: 0.0107\n",
      "F1 = 0.4619, ER = 0.6244 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 43/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1519 - model_2_loss: 0.1675 - model_1_loss: 0.0107\n",
      "F1 = 0.4516, ER = 0.6336 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 44/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1508 - model_2_loss: 0.1664 - model_1_loss: 0.0108\n",
      "F1 = 0.4636, ER = 0.6239 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 45/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1498 - model_2_loss: 0.1653 - model_1_loss: 0.0108\n",
      "F1 = 0.4644, ER = 0.6200 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 46/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1487 - model_2_loss: 0.1640 - model_1_loss: 0.0110\n",
      "F1 = 0.4713, ER = 0.6147 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 47/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1467 - model_2_loss: 0.1618 - model_1_loss: 0.0110\n",
      "F1 = 0.4659, ER = 0.6266 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 48/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1458 - model_2_loss: 0.1608 - model_1_loss: 0.0111\n",
      "F1 = 0.4525, ER = 0.6391 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 49/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1438 - model_2_loss: 0.1586 - model_1_loss: 0.0110\n",
      "F1 = 0.4743, ER = 0.6028 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 50/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1440 - model_2_loss: 0.1587 - model_1_loss: 0.0109\n",
      "F1 = 0.4714, ER = 0.6077 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 51/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1433 - model_2_loss: 0.1580 - model_1_loss: 0.0110\n",
      "F1 = 0.4645, ER = 0.6163 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 52/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1417 - model_2_loss: 0.1562 - model_1_loss: 0.0111\n",
      "F1 = 0.4619, ER = 0.6267 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 53/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1427 - model_2_loss: 0.1573 - model_1_loss: 0.0112\n",
      "F1 = 0.4669, ER = 0.6171 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 54/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1392 - model_2_loss: 0.1534 - model_1_loss: 0.0110\n",
      "F1 = 0.4569, ER = 0.6161 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 55/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1376 - model_2_loss: 0.1517 - model_1_loss: 0.0112\n",
      "F1 = 0.4628, ER = 0.6201 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 56/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1368 - model_2_loss: 0.1508 - model_1_loss: 0.0112\n",
      "F1 = 0.4673, ER = 0.6198 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 57/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1356 - model_2_loss: 0.1494 - model_1_loss: 0.0113\n",
      "F1 = 0.4681, ER = 0.6167 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 58/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1345 - model_2_loss: 0.1482 - model_1_loss: 0.0112\n",
      "F1 = 0.4719, ER = 0.6032 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 59/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1336 - model_2_loss: 0.1472 - model_1_loss: 0.0113\n",
      "F1 = 0.4721, ER = 0.6045 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 60/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1336 - model_2_loss: 0.1471 - model_1_loss: 0.0112\n",
      "F1 = 0.4683, ER = 0.6051 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 61/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1306 - model_2_loss: 0.1439 - model_1_loss: 0.0113\n",
      "F1 = 0.4729, ER = 0.5984 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 62/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1299 - model_2_loss: 0.1431 - model_1_loss: 0.0113\n",
      "F1 = 0.4562, ER = 0.6290 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 63/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1297 - model_2_loss: 0.1428 - model_1_loss: 0.0114\n",
      "F1 = 0.4562, ER = 0.6314 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 64/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1278 - model_2_loss: 0.1407 - model_1_loss: 0.0115\n",
      "F1 = 0.4642, ER = 0.6071 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 65/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1266 - model_2_loss: 0.1394 - model_1_loss: 0.0114\n",
      "F1 = 0.4706, ER = 0.5975 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 66/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1264 - model_2_loss: 0.1392 - model_1_loss: 0.0113\n",
      "F1 = 0.4622, ER = 0.6253 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 67/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1276 - model_2_loss: 0.1405 - model_1_loss: 0.0113\n",
      "F1 = 0.4712, ER = 0.6086 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 68/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1247 - model_2_loss: 0.1373 - model_1_loss: 0.0114\n",
      "F1 = 0.4693, ER = 0.6078 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 69/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1241 - model_2_loss: 0.1367 - model_1_loss: 0.0115\n",
      "F1 = 0.4693, ER = 0.6089 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 70/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1224 - model_2_loss: 0.1348 - model_1_loss: 0.0115\n",
      "F1 = 0.4666, ER = 0.6110 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 71/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1213 - model_2_loss: 0.1335 - model_1_loss: 0.0115\n",
      "F1 = 0.4720, ER = 0.5926 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 72/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1216 - model_2_loss: 0.1338 - model_1_loss: 0.0115\n",
      "F1 = 0.4634, ER = 0.6043 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 73/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1205 - model_2_loss: 0.1326 - model_1_loss: 0.0116\n",
      "F1 = 0.4607, ER = 0.6141 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 74/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1192 - model_2_loss: 0.1311 - model_1_loss: 0.0115\n",
      "F1 = 0.4601, ER = 0.6252 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 75/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1182 - model_2_loss: 0.1301 - model_1_loss: 0.0117\n",
      "F1 = 0.4689, ER = 0.5967 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 76/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1169 - model_2_loss: 0.1286 - model_1_loss: 0.0116\n",
      "F1 = 0.4672, ER = 0.6046 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 77/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1174 - model_2_loss: 0.1291 - model_1_loss: 0.0117\n",
      "F1 = 0.4665, ER = 0.6031 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 78/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1161 - model_2_loss: 0.1277 - model_1_loss: 0.0116\n",
      "F1 = 0.4626, ER = 0.6047 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 79/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1147 - model_2_loss: 0.1261 - model_1_loss: 0.0117\n",
      "F1 = 0.4703, ER = 0.5873 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 80/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1133 - model_2_loss: 0.1246 - model_1_loss: 0.0117\n",
      "F1 = 0.4693, ER = 0.5980 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 81/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1128 - model_2_loss: 0.1241 - model_1_loss: 0.0117\n",
      "F1 = 0.4717, ER = 0.5919 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 82/101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1130 - model_2_loss: 0.1242 - model_1_loss: 0.0117\n",
      "F1 = 0.4724, ER = 0.5980 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 83/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1109 - model_2_loss: 0.1220 - model_1_loss: 0.0117\n",
      "F1 = 0.4649, ER = 0.6004 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 84/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1093 - model_2_loss: 0.1201 - model_1_loss: 0.0117\n",
      "F1 = 0.4659, ER = 0.5903 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 85/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1110 - model_2_loss: 0.1220 - model_1_loss: 0.0118\n",
      "F1 = 0.4686, ER = 0.6015 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 86/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1098 - model_2_loss: 0.1207 - model_1_loss: 0.0119\n",
      "F1 = 0.4697, ER = 0.5809 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 87/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1077 - model_2_loss: 0.1184 - model_1_loss: 0.0118\n",
      "F1 = 0.4649, ER = 0.6001 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 88/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1088 - model_2_loss: 0.1195 - model_1_loss: 0.0119\n",
      "F1 = 0.4743, ER = 0.5759 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 89/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1080 - model_2_loss: 0.1186 - model_1_loss: 0.0118\n",
      "F1 = 0.4707, ER = 0.5854 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 90/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1053 - model_2_loss: 0.1157 - model_1_loss: 0.0119\n",
      "F1 = 0.4567, ER = 0.6191 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 91/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1053 - model_2_loss: 0.1157 - model_1_loss: 0.0120\n",
      "F1 = 0.4598, ER = 0.6125 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 92/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1044 - model_2_loss: 0.1147 - model_1_loss: 0.0120\n",
      "F1 = 0.4688, ER = 0.5981 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 93/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1042 - model_2_loss: 0.1144 - model_1_loss: 0.0119\n",
      "F1 = 0.4698, ER = 0.5852 - Best val F1s: 0.4743 (36)\n",
      "\n",
      "Epoch 94/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1032 - model_2_loss: 0.1133 - model_1_loss: 0.0118\n",
      "F1 = 0.4777, ER = 0.5741 -  Best val F1s: 0.4777 (IMPROVEMENT, saving)\n",
      "\n",
      "Epoch 95/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1033 - model_2_loss: 0.1134 - model_1_loss: 0.0118\n",
      "F1 = 0.4610, ER = 0.5967 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 96/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1016 - model_2_loss: 0.1116 - model_1_loss: 0.0118\n",
      "F1 = 0.4736, ER = 0.5846 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 97/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1008 - model_2_loss: 0.1106 - model_1_loss: 0.0120\n",
      "F1 = 0.4631, ER = 0.6072 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 98/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1011 - model_2_loss: 0.1110 - model_1_loss: 0.0120\n",
      "F1 = 0.4621, ER = 0.6018 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 99/101\n",
      "60000/60000 [==============================] - 68s 1ms/step - loss: 0.1000 - model_2_loss: 0.1097 - model_1_loss: 0.0121\n",
      "F1 = 0.4676, ER = 0.5790 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 100/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0988 - model_2_loss: 0.1085 - model_1_loss: 0.0120\n",
      "F1 = 0.4704, ER = 0.5774 - Best val F1s: 0.4777 (93)\n",
      "\n",
      "Epoch 101/101\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.0987 - model_2_loss: 0.1083 - model_1_loss: 0.0120\n",
      "F1 = 0.4701, ER = 0.5882 - Best val F1s: 0.4777 (93)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_frames = mel_val.shape[1]\n",
    "sequence_samples = int(sequence_time*sr)\n",
    "\n",
    "# Build model\n",
    "print('\\nBuilding model...')\n",
    "\n",
    "model_mel = MST(mel_bands,sequence_samples,audio_win,audio_hop)  \n",
    "model_cnn = build_custom_cnn(n_freq_cnn=mel_bands, n_frames_cnn=sequence_frames,large_cnn=large_cnn)\n",
    "\n",
    "# Init with best weigths\n",
    "model_mel.load_weights(\"../../sed_endtoend/mst/weights_best.hdf5\")\n",
    "model_cnn.load_weights(\"../../sed_endtoend/cnn/weights_best.hdf5\")\n",
    "\n",
    "model = concatenate(sequence_frames,audio_win,model_cnn,model_mel,sequence_samples=sequence_samples,frames=frames)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "if resume:\n",
    "    print('Loading best weights and resuming...')\n",
    "    weights_best_file = os.path.join(expfolder, 'weights_best.hdf5')\n",
    "    model.load_weights(weights_best_file)\n",
    "\n",
    "# Fit model\n",
    "print('\\nFitting model...')\n",
    "\n",
    "if resume:\n",
    "    f1s_best = resume_f1_best\n",
    "\n",
    "metrics_callback = MetricsCallback(x_val, [y_val,mel_val], 0, 0, os.path.join(expfolder, 'weights_best.hdf5'))\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "#losses_factor = K.variable(1/16.)\n",
    "#alpha = K.variable(0.0)\n",
    "#beta = K.variable(1.0)\n",
    "#bt_callback = BT_strategy(alpha,beta, a=30, b=5, W_end=0.5)\n",
    "\n",
    "model.compile(loss=['binary_crossentropy','mean_squared_error'],loss_weights=[0.9, 0.1],optimizer=opt)\n",
    "\n",
    "history = model.fit(x=x_train, y=[y_train,mel_train], batch_size=2*batch_size,\n",
    "                            epochs=epochs, verbose=fit_verbose,\n",
    "                            validation_split=0.0,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[metrics_callback,csv_logger])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
