from keras import optimizers
from keras.callbacks import CSVLogger
import numpy as np
import os
import librosa
import glob
#import matplotlib
#matplotlib.use('Agg')
#import matplotlib.pyplot as plt

import sys
sys.path.insert(0,'..')
from sed_endtoend.mst.model import MST
from sed_endtoend.callbacks import *
from sed_endtoend.gen_mel_filters import mel_filters
from sed_endtoend.data_generator import DataGenerator

os.environ["CUDA_VISIBLE_DEVICES"]="0"

# files parameters
Nfiles = None
resume = False
load_subset = Nfiles

# audio parameters
sr = 22050
sequence_time = 1.0
sequence_hop_time = 1.0
audio_hop = 512
audio_win = 1024
n_fft = 1024
normalize_data = 'minmax' # Paper de MST
get_annotations = True
mel_bands = 128
htk = True
normalize_energy = False

# training
learning_rate = 0.001 # Paper de MST
epochs = 101
batch_size = 64
sed_early_stopping = 100
epoch_limit = None
fit_verbose = True
fine_tuning = False

#model
large_cnn = True
frames = False # False para MST


label_list = (['air_conditioner', 'car_horn', 'children_playing',
               'dog_bark', 'drilling', 'engine_idling', 'gun_shot',
               'jackhammer', 'siren', 'street_music'])         

# Create output folders
expfolder = '../sed_endtoend/mst'

feature_folder = '/data_ssd/users/pzinemanas/maestria/URBAN-SED/audio22050'
#feature_folder = '../../MedleyDB/22050'
label_folder='/data_ssd/users/pzinemanas/maestria/URBAN-SED/annotations'
mel_folder='/data_ssd/users/pzinemanas/maestria/URBAN-SED/features_librosa'

alpha = 10**8 #REF del log
params = {'files_batch':20, 'path':feature_folder, 'sequence_time': sequence_time, 'sequence_hop_time':sequence_hop_time,'label_list':label_list,'alpha': alpha,'normalize_energy':normalize_energy,
          'audio_hop':audio_hop, 'audio_win':audio_win,'n_fft':n_fft,'sr':sr,'mel_bands':mel_bands,'normalize':normalize_data, 'frames':frames,'get_annotations':get_annotations}

sequence_frames = int(np.ceil(sequence_time*sr/audio_hop))

# Datasets
partition = {}# IDs
labels = {}# Labels

train_files = sorted(glob.glob(os.path.join(feature_folder,'train', '*.wav')))
val_files = sorted(glob.glob(os.path.join(feature_folder,'validate', '*.wav')))

if load_subset is not None:
    train_files = train_files[:load_subset]
    val_files = val_files[:load_subset]

train_labels = {}
train_mel = {}
val_labels = {}
val_mel = {}
mel_basis = librosa.filters.mel(sr,n_fft,mel_bands,htk=True)
print('Founding scaler')
for n,id in enumerate(train_files):
	labels[id] = os.path.join(label_folder, 'train',os.path.basename(id).replace('.wav','.txt'))
#	train_mel[id]  = os.path.join(mel_folder, 'train',os.path.basename(id).replace('.wav','.npy.gz'))
for id in val_files:
    labels[id] = os.path.join(label_folder, 'validate',os.path.basename(id).replace('.wav','.txt'))

params['train'] = True
# Generators
print('Making generators')
training_generator = DataGenerator(train_files, labels, **params)
scaler = training_generator.get_scaler()
#print('scaler',scaler)

params['scaler'] = scaler
params['train'] = False
params['sequence_hop_time'] = sequence_time

validation_generator = DataGenerator(val_files, labels, **params)

print('Getting data')

train_example,_,train_y_example,_ = training_generator.return_random()
val_example,_,val_y_example,_ = validation_generator.return_random()


x_val,_,y_val,_ = validation_generator.return_all()
x_train,_,y_train,_ = training_generator.return_all()

#if not frames:    
#    x_train = np.transpose(x_train,(0,2,1))    
#    x_val = np.transpose(x_val,(0,2,1))  
#    val_example = np.transpose(val_example,(0,2,1))     
print(x_train.shape)    
print(x_val.shape)     

print(x_val.shape,y_val[0].shape,y_val[1].shape)
sequence_frames = x_val.shape[1]
scaler2 = training_generator.get_scaler2()

mean= scaler2.mean_
scale = scaler2.scale_

# Build model

print('\nBuilding model...')

sequence_samples = int(sequence_time*sr)

model = MST(mel_bands,sequence_samples,audio_win,audio_hop)

model.summary()

opt = optimizers.Adam(lr=learning_rate)

if resume:
    print('Loading best weights and resuming...')
    weights_best_file = os.path.join(expfolder, 'weights_best.hdf5')
    model.load_weights(weights_best_file)

# Fit model
print('\nFitting model...')

if resume:
    f1s_best = resume_f1_best

#metrics_callback = MetricsCallback(x_val, y_val, 0, 0, os.path.join(expfolder, 'MST_weights_best.hdf5'))
#save_fig = SaveFigCallback(train_example,train_y_example,val_example,val_y_example,'./MST/')
csv_logger = CSVLogger(os.path.join(expfolder, 'training.log'))

model.compile(loss='mean_squared_error',optimizer=opt)

history = model.fit(x=x_train, y=y_train, batch_size=2*batch_size, #Borrar el 10!
                            epochs=epochs, verbose=fit_verbose,
                            validation_split=0.0,
                            shuffle=True,
                            #callbacks=[MyCallback(alpha, beta),metrics_callback,save_fig])
                            #callbacks=[save_fig,csv_logger])
                            callbacks=[csv_logger])
                            
model.save_weights(os.path.join(expfolder, 'weights_best.hdf5'))


